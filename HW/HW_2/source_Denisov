import numpy as np
from scipy.special import expit
from sklearn.metrics import log_loss


class LogReg():
    def __init__(self, lambda_1=0.0, lambda_2=1.0, gd_type='stochastic',
                 tolerance=1e-5, max_iter=2000, w0=None, alpha=1e-03):
        """
        lambda_1: L1 regularization param
        lambda_2: L2 regularization param
        gd_type: 'full' or 'stochastic'
        tolerance: for stopping gradient descent
        max_iter: maximum number of steps in gradient descent
        w0: np.array of shape (d) - init weights
        alpha: learning rate
        """
        self.lambda_1 = lambda_1
        self.lambda_2 = lambda_2
        self.gd_type = gd_type
        self.tolerance = tolerance
        self.max_iter = max_iter
        self.w0 = w0
        self.alpha = alpha
        self.w = None
        self.loss_history = None

    def fit(self, X, y):
        """
        X: np.array of shape (l, d)
        y: np.array of shape (l)
        ---
        output: self
        """
        self.loss_history = []
        if self.w0 is None:
            self.w = np.zeros(X.shape[1])
        else:
            self.w = self.w0
        i = 1
        while 1:
            w_grad = self.calc_gradient(X, y)
            w_new = self.w - self.alpha * w_grad
            if ((np.linalg.norm(self.w - w_new) > self.tolerance) & (i <= self.max_iter)):
                self.w = w_new
                i = i + 1
            else:
                break
        return self

    def predict_proba(self, X):
        """
        X: np.array of shape (l, d)
        ---
        output: np.array of shape (l, 2) where
        first column has probabilities of -1
        second column has probabilities of +1
        """
        if self.w is None:
            raise Exception('Not trained yet')
        else:
            y_pred = expit(np.dot(X, self.w))
            result = np.vstack((1 - y_pred.flatten(), y_pred.flatten()))
            return result.T

    def calc_gradient(self, X, y):
        """
        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)
        y: np.array of shape (l)
        ---
        output: np.array of shape (d)
        """
        sigmoid = expit(-1.0 * y * np.dot(X, self.w))
        yX = y[:, np.newaxis] * X
        sigmoid_yX = sigmoid[:, np.newaxis] * yX
        gradient = np.sum(sigmoid_yX, axis=0)
        w_new = -1.0 * gradient / (1.0 * X.shape[0]) + self.lambda_2 * self.w
        return w_new

    def calc_loss(self, X, y):
        """
        X: np.array of shape (l, d)
        y: np.array of shape (l)
        ---
        output: float
        """
        a = np.logaddexp(0, -y * np.dot(X, self.w))
        score = (1 / X.shape[0]) * np.sum(a, axis=0) + self.lambda_2 * np.sum(self.w ** 2) / 2
        return score
